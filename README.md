# AIIJC. Перевод

## Как подступиться?
Был первый вопрос, когда прочитали задачу. Потом вспомнили, что делали на прошлом этапе, и решили это использовать. Итак, если описывать общую схему работы решения, то она такая:
1. "Заморозить" то, что никак не кореллирует с темами, вроде чисел и слов по типу "сколько", "среднее арифметическое" и тд, а также знаки пунктуации.
2. Заменить ключевые слова задчи, являющиеся подлежащими, на подлежащие целевой темы
3. Заменить ключевые слова-дополнения на сочетающиеся с подобранными подлежащими.
4. Используя BERT или при помощи предобученных эмбеддингов с масками подставить глаголы.
5. Пофиксить падежи, числа, сделать большие буквы в начале предложения, короче - косметология.

## Заморозка
А имено, как понять, какие слова менять? На собранном датасете выделим [ключевые слова](Lisa/FinalKeywordsObject.ipynb) на основании косинусной разницы и сформируем списки ключевых слов для каждой из четырёх тем. Тогда в поступившей на обработку задаче мы можем проверить каждое слово на момент того, является ли оно контекстным. Слова, не входящие в списки ключевых, заморозим. Также отдельно создадим файлик с единицами измерения ("метр", "м", "килограмм") и с некоторыми ключевыми словами в задачках ("длина", "сколько", "ответ"), скопировав все эти слова с [сайта](http://standards.narod.ru/ok/okei.htm) с единицами измерений и с сайтов с нейтральными задачками (в которых нет ключевых слов ни по одной из тем) (например, с этого [сайта](https://multiurok.ru/files/gieomietrichieskiie-zadachi-2.html)): [файл](Mitya/units.txt), [файл](Mitya/keywords_tasks.txt)

## Подлежащие
Мы решили начинать изменения с подлежащего, так как его не сложно изменить на целевое, 
а сути задачки оно не обычно не меняет. 
Искали Эмпирически. В задачах существительные в именительном падеже -
почти всегда подлежащие, тогда используя pymorphy2 можно определять подлежащия.

Дальше можно просто поменять одно подлежащее на другое из другой категории, но можно сделать лучше:

Для примера возьмем в качестве 
* Действуйщего лица - `волейболист`
* Изначальная категория - `спорт`
* Целевая категория - `животные`

Заменяем наше подлежащее на слово, которое ближе всего к 
`волейболист - спорт + животное`, а так же является действующим лицом.

Для определения ближайшего слова искали косинусное расстояние между предобученными
эмбеддингами (использовали [navec](https://habr.com/ru/post/516098/)).

Всех действуйщих лиц предварительно нашли при помощи выделения ключевых слов и статистических методов.

## Дополнения
После того, как мы заменили главного героя, мы тем самым дали основу, так сказать якорь, 
от которого можно отталкиваться для дальнейшей генерации. И теперь можно было приступать к 
остальной стилизации текста.

### Предсказывать с помощью BERT

Пробовали обычный [bert-base-multilingual-uncased](https://revuze.it/blog/bert-nlp/), 
[RuBERT](https://arxiv.org/abs/1905.07213) и [RoBERTa](https://arxiv.org/abs/1907.11692)
Мы решили попробовать предсказывать необходимые ключевые слова при помощи берта.
BERT для каждой маски предсказывал вероятности того, что на месте этой маски стоит 
соответствующий токен. 

Мы домножали эти вероятности на расстояние (косинусное) от данного слова до темы,
в которую хотим перевести текст. Косинусное расстояние и вероятности мы предварительно нормировали в (0, 1).

К сожалению, данное решение имело несколько недостатков 
* Берт предсказывает адекватно высокии вероятности только для небольшого кол-ва слов (максимум 5 - 10).
Это, в принципе, логично, так как он обучался, чтобы выдавать один классный ответ, а следовательно, у нас очень мало слов, из которых можно выбирать и нужной нам темы в них зачастую нет
* Вторая проблема заключалась в том, что если мы убираем все ключевые слова, то ему просто не за что зацепиться.
Например, при попытке предсказать маски для `У Миши было 3 [MASK], 2 из них он отдал Даше. Сколько [MASK] у него осталось?`
BERT теряется и предсказывает какую-то несуразицу

В связи с тем, что мы так и не смогли решить эти 2 проблемы, мы отошли от идеи его дальнейшего использования.

### Предсказывать при помощи предобученных эмбеддингов

Тут мы пробовали несколько техник (идеи некоторых из них есть на сайте [RusVectores](https://rusvectores.org/ru/about/)):

* Так же, как и подлежащие, находить дополнения с помощью разности глобальных векторов (например, `мяч - спорт + животное`): этот подход часто выдавал не самые походящие по контексту слова, и почти всегда переведённые задачи выглядели как набор ключевых слов категории, в которую переводили (связей между словами не было)

* Пытаться подбирать такие слова, чтобы разность между уже найденным подлежащим и дополнениями была как можно ближе к разности между изначальным подлежащим и изначальными дополнениями (например, `мальчик` -- изначальное подлежащее, `мяч` -- изначальное дополнение, `белочка` -- новое подлежащее, тогда мы должны найти такое слово X, чтобы `мальчик - мяч` было примерно равно `белочка - X`). Такой подход учитывает связи между словами и довольно неплохо преобразует тексты.

* Также мы столкнулись с недостатком ключевых слов или с их плохим качеством и решили попробовать доставать ключевые слова из эмбеддингов. Если просто разбивать слова на 4 категории с помощью нахождения расстояния между векторами слов и категорий (например, расстояние между `мяч` и `спорт`), мы получим очень плохую классификацию, ведь нет слов, которые не относятся ни к одной из категорий. Также у некоторых ключевых слов достаточно большое расстояние со словом этой категории, поэтому отсекать слова по значению расстояния не получится. Но мы придумали, как разбивать слова лучше. Нужно смотреть сразу на несколько расстояний до слов (например, расстояния от слова `мяч` до слов `спорт`, `победа`, `соревнование`). Таким образом, если слово является очень близким со всеми словами из категории и довольно далёко от остальных категорий, его можно смело считать ключевым. Таким способом мы находили очень качественные ключевые слова.

Плюсами данного подхода является 
1. Мы предсказываем только существительные, которые точно являются ключевыми словами
2. Не можем предсказать что-то несуразное, вроде запятой
3. Учитываем тему и слово, которое используются в качестве главного героя

## Глаголы и другие части речи

Мы хотели заменять их с помощью берта, но эта идея так же провалилась, как и с дополнениями, поэтому мы решили, что их мы тоже будем обрабатывать эмбеддингами

## Доводим всё до идеала
Для того, чтобы согласовать все падежи, роды, склонения и т.д., которые стоят неправильно 
в связи с использованием начальных форм слов, мы переводили текст на английский, а потом обратно на русский.

Для перевода использовали https://www.m-translate.ru/translator/text#text=test&direction=ru-en, так как он не имеет ограничения на количество запросов

# AIIJC. Классификация

## Коротко о работе:
Всю проделанную нами работу можно разделить на несколько основных шагов:
1. Собрать данные
2. Очистить и нормализовать данные
3. Эксперименты, эксперименты, и ещё раз эээээкспериименты
4. Создание телеграм-бота

Расскажем поподробнее о каждом из этих шагов

## Сбор данных

Для обучения наших моделей мы решили собрать дополнительные данные, так как тексты из википедии не очень похожи на школьные задания.

1. **Сборники задач**
После изучения тестового датасета мы поняли, что нашими целевыми заданиями являются задачки по математике для детей начальной школы.
Это задачки из разряда "У белочки было 3 орешка, два она потяряла. Сколько орехов осталось у белочки?". Поэтому за основу создания датасета были взяты 
[сборники](https://github.com/sergak0/AIIJC/tree/main/Sergey/data_zadachi) задач по математике для 1-5 класса. Их мы взяли из открытых источников в интернете.

После того как данные были скачаны и [привидены в нормальный вид](https://github.com/sergak0/AIIJC/blob/main/Sergey/Additional_dataset_creating.ipynb) (достали сами условия задач), мы прогнали их байесовской модели, обученной к тому времени на википедии. Но, к сожалению, результаты обучения на таком датасете нас не порадовали и поэтому пришлось размечать обучающие данные ручками.

2. **Интересные факты**
Также мы подумали, что различные интересные факты, которые можно найти в интернете (например, по запросу "Итересные факты о животных"), довольно чистые и достаточно неплохо походят на задачи из тестового датасета, а также их не надо размечать. Мы ручками собрали [их](https://github.com/sergak0/AIIJC/blob/main/facts2.csv), заходя на различные сайты.

## Предобработка данных

- Токенизация
- Выброс чисел
- Лемматизация с помощью pymorphy2
- Очистка от стоп-слов (не несущих смысла, вроде местоимений и предлогов)
- Выделение биграмм


## Исследование

В ходе решения кейса мы перепробовали несколько подходов и архитектур, расскажем поподробнее о каждом и них.
1. **Классические алгоритмы машинного обучения:**
- [Наивный байес](Lisa/nb_usage.ipynb): был лидерм по точности, пока обучали только на википедии. 
Объясняется, вероятно, тем, что из-за отсутствия какой либо структуры в тексте остальные модели плохо с ним работали. 
А Байес, как статистическая модель, выезжал за счёт разнообразия лексики с статьях википедии.
Но после дообучения всех моделей на задачах и фактах стал отставать от своих собратьев, так как лексику не сильно пополнил, а структуру так и не усвоил
- [Логистическая регрессия на one-hot encoded dictionary](Lisa/baseline_logreg.ipynb): плохое качество, тк вектора очень разреженные
- [Логистическая регрессия на tf-idf ecncoded](https://github.com/sergak0/AIIJC/blob/main/Sergey/logistic_regression.py): качество тоже так себе. [Usage](https://github.com/sergak0/AIIJC/blob/main/Sergey/Logistic_regression_usage.ipynb)
- [Логистическая регрессия на Word2Vec](Lisa/baseline_w2v_logreg.ipynb): лишь немного уступал Байесу на вики-данных, на чистых данных перегнал его и является неплохим бейзлайном, 
тк учитывает общий смысловой вектор текста
- [Логистическая регрессия/полносвязная нейронная сеть на LDA](Lisa/baseline_lda_fullt.ipynb): в числе лидеров на вики-данных, потрясающе самостоятельно выделяет нужные нам 4 класса и пятый со стоп-словами, не несущими смысла. 
Использовали для создания хорошего словаря [стоп-слов](Lisa/all_stopwords.txt)
- Всё то же самое, но смотрим только на [ключевые слова](Lisa/KeywordBased.ipynb), выделенные на основе cosine similarity векторов: точность модели почти не меняется, таким образом, мы можем "сжать" данные и заставить конечный алгоритм работать быстрее.

2. **Трансформеры**
- [BERT](https://github.com/sergak0/AIIJC/tree/main/Mitya): мы использовали bert-base-multilingual-uncased. На википедии очень плохо обучался, потому что данные из википедии очень грязные. На дополнительных данных, которые мы собирали, показал себя лучше всех моделей. Пробовали обучать на сборниках задач, на фактах, на сборниках задач и фактах вместе. Последнее дало лучший результат. С помощью output_attentions=True мы выводили ключевые слова.
 
 3. **Подходы и открытия**
 - В качестве входный данных использовать не текст, а [ключевые слова](Lisa/KeywordBased.ipynb), выделенные на основе cosine similarity векторов слов: 
 точность модели почти не меняется, таким образом, мы можем "сжать" данные и заставить конечный алгоритм работать быстрее.
