{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "CreateMask.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpKNYvQQE08G"
      },
      "source": [
        "# Зависимости\n",
        "\n"
      ],
      "id": "hpKNYvQQE08G"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rckox-oHcZOe",
        "outputId": "0da605b6-eb17-4255-aa04-09973375f17f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "rckox-oHcZOe",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqIE6q9ksTge",
        "outputId": "d1eea075-c519-4d87-c3b7-894d0a325bd8"
      },
      "source": [
        "! pip install numpy\n",
        "! pip install pymorphy2[fast]\n",
        "! pip install nltk\n"
      ],
      "id": "xqIE6q9ksTge",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting pymorphy2[fast]\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 10.7 MB/s \n",
            "\u001b[?25hCollecting DAWG>=0.8\n",
            "  Downloading DAWG-0.8.0.tar.gz (371 kB)\n",
            "\u001b[K     |████████████████████████████████| 371 kB 53.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.8.0-cp37-cp37m-linux_x86_64.whl size=858796 sha256=7e4883e39046e66ee6d012406a23ef6c4a0e994bca0a5890349b00b720dc81a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/51/a4/2de41ff197786537075027c27b479a38da92f50abc86634445\n",
            "Successfully built DAWG\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2, DAWG\n",
            "Successfully installed DAWG-0.8.0 dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b15af6c3"
      },
      "source": [
        "import random\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import shutil\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import warnings \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from PIL import Image\n",
        "from IPython.display import clear_output\n",
        "from typing import List\n",
        "import nltk\n",
        "import string\n",
        "import pymorphy2\n",
        "import codecs\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "id": "b15af6c3",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjCGXv1dtVw3"
      },
      "source": [
        "warnings.filterwarnings(action='ignore',category=UserWarning, module='gensim')  \n",
        "warnings.filterwarnings(action='ignore',category=FutureWarning, module='gensim')  \n",
        "warnings.filterwarnings(action='ignore',category=DeprecationWarning, module='gensim')\n",
        "warnings.filterwarnings(action='ignore',category=DeprecationWarning, module='smart_open') \n",
        "warnings.filterwarnings(action='ignore',category=DeprecationWarning, module='sklearn')\n",
        "warnings.filterwarnings(action='ignore',category=DeprecationWarning, module='scipy')    \n",
        "warnings.filterwarnings(action='ignore',category=DeprecationWarning, module='pymorphy2')"
      ],
      "id": "RjCGXv1dtVw3",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo_synQqmk89",
        "outputId": "b7834153-2778-4117-ac80-88f021b0d158"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopWordsRu = set(stopwords.words(\"russian\")) \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "Bo_synQqmk89",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdkwmNGLHSAJ"
      },
      "source": [
        "class MaskCreator():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.morph = pymorphy2.MorphAnalyzer()\n",
        "    self.tokenizer = nltk.WordPunctTokenizer()\n",
        "    self.bigram_mod = gensim.models.Phrases.load(\"/content/drive/MyDrive/aiijc_sber/bigram_model.pkl\")\n",
        "    self.keywords = set()\n",
        "    for category in ['literature', 'music', 'sport', 'animals']:\n",
        "      with open(\"/content/drive/MyDrive/aiijc_sber/true_keywords_nouns_\" + category+ \".txt\", 'r') as file:\n",
        "        self.keywords |= set([line.strip() for line in file.readlines()])\n",
        "\n",
        "  def make_bigrams(self, doc):\n",
        "      return self.bigram_mod[doc]\n",
        "\n",
        "  def mask(self, text) -> (str, List[str]):\n",
        "    masks_dict = []\n",
        "    tokens = self.tokenizer.tokenize(text.lower())\n",
        "    tokens_normal = [self.morph.parse(w)[0].normal_form for w in tokens]\n",
        "    tokens_bigrammed = self.make_bigrams(tokens_normal)\n",
        "    \n",
        "    if len(tokens_bigrammed) < len(tokens):\n",
        "      ind_go = 0\n",
        "      for i in range(len(tokens_bigrammed)):\n",
        "        if tokens_normal[ind_go] != tokens_bigrammed[i]:\n",
        "          tokens = tokens[:ind_go] + [tokens_bigrammed[i]] + tokens[ind_go+2:]\n",
        "          ind_go += 2\n",
        "        else:\n",
        "          ind_go += 1\n",
        "\n",
        "    for ind, token in enumerate(tokens):\n",
        "      word = self.morph.parse(token.lower())[0].normal_form\n",
        "      if word in self.keywords:\n",
        "        if word not in masks_dict:\n",
        "          masks_dict.append(word)\n",
        "        tokens[ind] = 'mask' + str(masks_dict.index(word, 0))\n",
        "    text = nltk.tokenize.treebank.TreebankWordDetokenizer().detokenize(tokens)\n",
        "    text = text.replace(' .', '.')\n",
        "    return text, masks_dict"
      ],
      "id": "GdkwmNGLHSAJ",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l44kEXwuMt-w",
        "outputId": "3720a52e-d272-4513-dd1d-4f4e02e0ca24"
      },
      "source": [
        "MaskCreator().mask(\"Глобус - модель земного шара. В морях плавают корабли.\")"
      ],
      "id": "l44kEXwuMt-w",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('глобус - модель земной_шар. в морях плавают mask0.', ['корабль'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}