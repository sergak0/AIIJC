{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPD5xkHjefGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c81717-2d5a-4ab5-8b6c-fbd1c35bea44"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTfGP2b6Adn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff67753b-9bfc-4703-9829-9bb99c7a8b56"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install pymorphy2[fast]\n",
        "!pip install navec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pymorphy2[fast] in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Requirement already satisfied: DAWG>=0.8 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.8.0)\n",
            "Requirement already satisfied: navec in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRYkmf3b98vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf0f600-dc4a-4233-b05e-72e7950d99c8"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import AdamW, BertForMaskedLM\n",
        "import torch\n",
        "import numpy as np\n",
        "import pymorphy2\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from navec import Navec\n",
        "from typing import List\n",
        "import gensim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import KeyedVectors\n",
        "import scipy.spatial.distance as cos_dist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from sklearn.metrics import f1_score\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(44)\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "nltk.download('punkt')\n",
        "navec = Navec.load(save_path + '/navec_hudlit_v1_12B_500K_300d_100q.tar')\n",
        "save_path = '/content/drive/MyDrive'"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjpegAf1_GJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa71911c-2c60-4518-d65a-6b3f6d719a19"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        " \n",
        "if device == torch.device('cpu'):\n",
        "    print('Using cpu')\n",
        "else:\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print('Using {} GPUs'.format(torch.cuda.get_device_name(0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6C6raB6KhYT"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DRPppmjzk7D"
      },
      "source": [
        "categories = (\"животные\", \"музыка\", \"спорт\", \"литература\")\n",
        "categories_eng = (\"animals\", \"music\", \"sport\", \"literature\")\n",
        "\n",
        "nouns_actors = []\n",
        "for cat in categories_eng:\n",
        "  with open(save_path + f'/keywords/actors/true_keywords_nouns_actors_{cat}.txt', 'r') as f:\n",
        "    words = f.read()\n",
        "  nouns_actors.append(words.split('\\n')[:-1])\n",
        "\n",
        "nouns = []\n",
        "for ind, cat in enumerate(categories_eng):\n",
        "  with open(save_path + f'/keywords/nouns/true_keywords_nouns_{cat}.txt', 'r') as f:\n",
        "    words = f.read()\n",
        "  words = words.split('\\n')[:-1]\n",
        "  nouns.append(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XtCRcXnMmzI"
      },
      "source": [
        "class MaskCreator():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.tokenizer = nltk.WordPunctTokenizer()\n",
        "    self.bigram_mod = gensim.models.Phrases.load(save_path + '/bigram_model.pkl')\n",
        "    self.keywords = set()\n",
        "    self.cat_keywords = [set() for i in range(4)]\n",
        "    for ind, cat in enumerate(categories_eng):\n",
        "      with open(save_path + f'/keywords/nouns/true_keywords_nouns_{cat}.txt', 'r') as file:\n",
        "        self.keywords |= set([line.strip() for line in file.readlines()])\n",
        "        self.cat_keywords[ind] |= set([line.strip() for line in file.readlines()])\n",
        "\n",
        "  def make_bigrams(self, doc):\n",
        "      return self.bigram_mod[doc]\n",
        "\n",
        "  def mask(self, text, category=4) -> (str, List[str]):\n",
        "    masks_dict = []\n",
        "    tokens = self.tokenizer.tokenize(text.lower())\n",
        "    tokens_normal = [morph.parse(w)[0].normal_form for w in tokens]\n",
        "    tokens_bigrammed = self.make_bigrams(tokens_normal)\n",
        "    \n",
        "    if len(tokens_bigrammed) < len(tokens):\n",
        "      ind_go = 0\n",
        "      for i in range(len(tokens_bigrammed)):\n",
        "        if tokens_normal[ind_go] != tokens_bigrammed[i]:\n",
        "          tokens = tokens[:ind_go] + [tokens_bigrammed[i]] + tokens[ind_go+2:]\n",
        "          ind_go += 2\n",
        "        else:\n",
        "          ind_go += 1\n",
        "\n",
        "    if category == 4:\n",
        "      now_keywords = self.keywords\n",
        "    else:\n",
        "      now_keywords = self.cat_keywords[category]\n",
        "\n",
        "    for ind, token in enumerate(tokens):\n",
        "      word = morph.parse(token.lower())[0].normal_form\n",
        "      if word in now_keywords:\n",
        "        if word not in masks_dict:\n",
        "          masks_dict.append(word)\n",
        "        tokens[ind] = 'mask' + str(masks_dict.index(word, 0))\n",
        "    text = nltk.tokenize.treebank.TreebankWordDetokenizer().detokenize(tokens)\n",
        "    text = text.replace(' .', '.')\n",
        "    return text, masks_dict"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFXxG2HIOXtg",
        "outputId": "859f8949-3bc0-4ae4-e60d-b0e887d25dae"
      },
      "source": [
        "MaskCreator().mask('Книги. книга. Футбольное поле, футбольных полей, бассейн бассейне, олимпийская игра.')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('mask0. mask0. футбольное mask1, футбольных mask1, mask2 mask2, олимпийский_игра.',\n",
              " ['книга', 'поле', 'бассейн'])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omtxV_FM55zt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQVwG9Gz552O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tP0P_cr555Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkfP_TBT56S_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "AsQiq33EyjyK",
        "outputId": "0417ed79-ba25-4f64-ea37-607956d2be4f"
      },
      "source": [
        "df = pd.read_csv(save_path + '/facts2.csv')\n",
        "df_new = df\n",
        "df = pd.read_csv(save_path + '/dataset_disclosed.csv', sep=';')\n",
        "df = df.drop('id', 1)\n",
        "df = df.rename(columns={'task': 'texts', 'category': 'labels'})\n",
        "df.labels = df.labels.replace('животные', 0).replace('музыка', 1).replace('спорт', 2).replace('литература', 3)\n",
        "df_new2 = df\n",
        "df = pd.read_csv(save_path + '/full_marked_dataset.csv')\n",
        "df = df.drop('Unnamed: 0', 1)\n",
        "df = df.rename(columns={'text': 'texts', 'ans': 'labels'})\n",
        "df.labels = df.labels.replace('животные', 0).replace('музыка', 1).replace('спорт', 2).replace('литература', 3).replace('неизвестно', 4)\n",
        "df = df.drop([i for i in range(len(df.labels)) if df.labels[i] == 4], 0)\n",
        "df = df.reset_index(drop=True)\n",
        "for i in range(len(df_new)):\n",
        "  df.loc[-i-1] = df_new.loc[i]\n",
        "df = df.reset_index(drop=True)\n",
        "for i in range(len(df_new2)):\n",
        "  df.loc[-i-1] = df_new.loc[i]\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\t1.1. Летом в спортивный лагерь ходили 50 дет...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\t1.2. На концерте в летнем лагере ребята игра...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\t1.4. Во время летних соревнований по плавани...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\t1.5. В спортивную школу во время летних кани...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\t1.7. Самые крупные животные на нашей планете...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2417</th>\n",
              "      <td>2.Спорт не стоит на месте, а движется вперед в...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2418</th>\n",
              "      <td>3. Активные спортивные занятия положительно вл...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2419</th>\n",
              "      <td>4. Спорт — лучшая профилактика депрессии, диаб...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2420</th>\n",
              "      <td>5. Для пожилых людей самыми полезными видами с...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421</th>\n",
              "      <td>6. Бокс был узаконен как вид спорта только в 1...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2422 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  texts  labels\n",
              "0     \\t1.1. Летом в спортивный лагерь ходили 50 дет...       2\n",
              "1     \\t1.2. На концерте в летнем лагере ребята игра...       1\n",
              "2     \\t1.4. Во время летних соревнований по плавани...       2\n",
              "3     \\t1.5. В спортивную школу во время летних кани...       2\n",
              "4     \\t1.7. Самые крупные животные на нашей планете...       0\n",
              "...                                                 ...     ...\n",
              "2417  2.Спорт не стоит на месте, а движется вперед в...       2\n",
              "2418  3. Активные спортивные занятия положительно вл...       2\n",
              "2419  4. Спорт — лучшая профилактика депрессии, диаб...       2\n",
              "2420  5. Для пожилых людей самыми полезными видами с...       2\n",
              "2421  6. Бокс был узаконен как вид спорта только в 1...       2\n",
              "\n",
              "[2422 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "olr4Q8c_hhBF",
        "outputId": "6b72c71e-6263-4a4e-f9b3-a061bd9941b9"
      },
      "source": [
        "with open(save_path + '/aiijc_comand_data.json', 'r') as f :\n",
        "  data = json.load(f)\n",
        "df_wiki = pd.DataFrame(data)\n",
        "\n",
        "df_wiki"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>links</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>животные</th>\n",
              "      <td>[Живо́тные (лат. Animalia) — традиционно (со в...</td>\n",
              "      <td>[Волосатики, Национальная парламентская библио...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>музыка</th>\n",
              "      <td>[Му́зыка (греч. μουσική, субстантивированное п...</td>\n",
              "      <td>[Мини-альбом, Волчья квинта, Ре (нота), Музыка...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>спорт</th>\n",
              "      <td>[Спорт (англ. sport, сокращение от первоначаль...</td>\n",
              "      <td>[Олимпийская хартия, Болельщик, Ольмеки, Шаоли...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>литература</th>\n",
              "      <td>[Литерату́ра (лат. lit(t)eratura, — написанное...</td>\n",
              "      <td>[Детская литература, Эфиопская литература, Сов...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        texts                                              links\n",
              "животные    [Живо́тные (лат. Animalia) — традиционно (со в...  [Волосатики, Национальная парламентская библио...\n",
              "музыка      [Му́зыка (греч. μουσική, субстантивированное п...  [Мини-альбом, Волчья квинта, Ре (нота), Музыка...\n",
              "спорт       [Спорт (англ. sport, сокращение от первоначаль...  [Олимпийская хартия, Болельщик, Ольмеки, Шаоли...\n",
              "литература  [Литерату́ра (лат. lit(t)eratura, — написанное...  [Детская литература, Эфиопская литература, Сов..."
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66V025_2IT03",
        "outputId": "1968c090-03b2-4fcc-c6bb-01a53e7c2b59"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя ')\n",
        "for ind, text in tqdm(enumerate(df_wiki['texts'])):\n",
        "  for sentence in text:\n",
        "    sentence = sentence.lower().replace('\\n', ' ')\n",
        "    sentence = ''.join(c for c in sentence if c in alphabet)\n",
        "    sentence = sentence.split()\n",
        "    for word in sentence:\n",
        "      #word = morph.parse(word)[0].normal_form\n",
        "      if not word in navec:\n",
        "        continue\n",
        "      X.append(navec[word])\n",
        "      #Y.append([0 if i != ind else 1 for i in range(4)])\n",
        "      Y.append(ind)\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [00:36,  9.14s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FN5xHXXy0rb",
        "outputId": "70067300-12d7-4845-9a63-0feb245b415b"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя ')\n",
        "for ind, text in tqdm(enumerate(df['texts'])):\n",
        "  label = df['labels'][ind]\n",
        "  text = text.lower().replace('\\n', ' ')\n",
        "  text = ''.join(c for c in text if c in alphabet)\n",
        "  text = text.split()\n",
        "  for word in text:\n",
        "    #word = morph.parse(word)[0].normal_form\n",
        "    if not word in navec:\n",
        "      continue\n",
        "    X.append(navec[word])\n",
        "    Y.append([0 if i != label else 1 for i in range(4)])\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [00:00, 12.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6b8C1fr1Ryq",
        "outputId": "1889b777-2d33-4702-ad7d-fd4613c210e2"
      },
      "source": [
        "X.shape, Y.shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3026268, 300), (3026268,))"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aYyF90OtRDr"
      },
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "word_model = Sequential()\n",
        "word_model.add(Dense(100, input_dim=300, activation='relu'))\n",
        "word_model.add(Dense(4, activation = 'softmax'))\n",
        "word_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1_m])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hMobXYe1grK",
        "outputId": "df59ad8c-5b0d-4cf6-9b5d-e1b928c933ac"
      },
      "source": [
        "word_model.fit(X, Y, epochs=10, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "11822/11822 [==============================] - 41s 3ms/step - loss: 1.2608 - accuracy: 0.3978 - f1_m: 0.1899\n",
            "Epoch 2/10\n",
            "11822/11822 [==============================] - 40s 3ms/step - loss: 1.2569 - accuracy: 0.4004 - f1_m: 0.1965\n",
            "Epoch 3/10\n",
            "11822/11822 [==============================] - 40s 3ms/step - loss: 1.2546 - accuracy: 0.4018 - f1_m: 0.1993\n",
            "Epoch 4/10\n",
            "11822/11822 [==============================] - 38s 3ms/step - loss: 1.2530 - accuracy: 0.4028 - f1_m: 0.2020\n",
            "Epoch 5/10\n",
            "11822/11822 [==============================] - 39s 3ms/step - loss: 1.2518 - accuracy: 0.4036 - f1_m: 0.2037\n",
            "Epoch 6/10\n",
            "11822/11822 [==============================] - 39s 3ms/step - loss: 1.2509 - accuracy: 0.4041 - f1_m: 0.2055\n",
            "Epoch 7/10\n",
            "11822/11822 [==============================] - 39s 3ms/step - loss: 1.2502 - accuracy: 0.4048 - f1_m: 0.2067\n",
            "Epoch 8/10\n",
            "11822/11822 [==============================] - 39s 3ms/step - loss: 1.2496 - accuracy: 0.4048 - f1_m: 0.2077\n",
            "Epoch 9/10\n",
            "11822/11822 [==============================] - 39s 3ms/step - loss: 1.2490 - accuracy: 0.4054 - f1_m: 0.2081\n",
            "Epoch 10/10\n",
            "11822/11822 [==============================] - 42s 4ms/step - loss: 1.2486 - accuracy: 0.4057 - f1_m: 0.2090\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff7a9cf7290>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl78fjvIlXhV",
        "outputId": "6a883149-d391-43a2-c790-4564622b5df0"
      },
      "source": [
        "word_model.predict(np.array([navec['турпоход']]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.30372334, 0.14590675, 0.38346455, 0.16690543]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZAOBaC354cQ"
      },
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NN, self).__init__()\n",
        "    self.l1 = nn.Linear(300, 100)\n",
        "    self.l2 = nn.ReLU()\n",
        "    self.l3 = nn.Linear(100, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.l3(x)\n",
        "    return x"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmQGWgIQ7xql",
        "outputId": "e77d108a-5256-45f5-d638-773afbfa389f"
      },
      "source": [
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n",
        "train_data = TensorDataset(X, Y)\n",
        "train_dataloader = DataLoader(\n",
        "  train_data,\n",
        "  sampler=RandomSampler(train_data),\n",
        "  batch_size=128\n",
        ")"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DogFt7Y_54Z_",
        "outputId": "32458179-5858-4525-89ec-9a191969ac10"
      },
      "source": [
        "word_model = NN()\n",
        "word_model = word_model.to(device)\n",
        "print(word_model)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN(\n",
            "  (l1): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (l2): ReLU()\n",
            "  (l3): Linear(in_features=100, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j6tpkLW54Xf"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss() \n",
        "criterion = criterion.to(device)\n",
        "optimizer = optim.Adam(word_model.parameters(), lr=1e-3)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_Nf4sHX537V",
        "outputId": "9979c461-5270-4460-b29f-b85c13e7dca2"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "train_loss_set = []\n",
        "\n",
        "word_model.train()\n",
        "\n",
        "for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_inputs, b_labels = batch\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  b_pred = word_model(b_inputs)\n",
        "\n",
        "  loss = criterion(b_pred, b_labels)\n",
        "\n",
        "  train_loss_set.append(loss.item())\n",
        "\n",
        "  loss.backward()\n",
        "  \n",
        "  optimizer.step()\n",
        "  \n",
        "  '''clear_output(True)\n",
        "  plt.plot(train_loss_set)\n",
        "  plt.title(\"Training loss\")\n",
        "  plt.xlabel(\"Batch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()'''"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23643/23643 [00:51<00:00, 459.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rsAU6uLRUD3",
        "outputId": "927f16db-c17d-4ed2-c1c8-9db507fc6772"
      },
      "source": [
        "word_model.eval()\n",
        "now = word_model(torch.tensor([navec['поход']]))\n",
        "now"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3445, -0.1988,  0.6808,  0.1844]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf2yaGnWSF9J",
        "outputId": "510bcb7b-876a-43fc-a8e4-ddb6b118c2fc"
      },
      "source": [
        "now_word = 'сколько'\n",
        "a = cos_dist.cosine(navec[now_word], navec['животные'])\n",
        "b = cos_dist.cosine(navec[now_word], navec['музыка'])\n",
        "c = cos_dist.cosine(navec[now_word], navec['спорт'])\n",
        "d = cos_dist.cosine(navec[now_word], navec['литература'])\n",
        "s = nn.Softmax()\n",
        "print(s(torch.tensor([1-a, 1-b, 1-c, 1-d])), 1-a)\n",
        "now = word_model(torch.tensor([navec[now_word]]))\n",
        "s(now)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.2512, 0.2569, 0.2375, 0.2544], dtype=torch.float64) 0.08583778887987137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1766, 0.2362, 0.2802, 0.3070]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIYITqJdVVfx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MV5oL4CQnNK"
      },
      "source": [
        "#sentence = '4.30. У слона пульс 20 ударов в минуту, а у паука на 40 ударов в минуту больше. Какой пульс у паука?'\n",
        "#sentence = '   1.1. Летом в спортивный лагерь ходили 50 детей, из них 9 девочек. Сколько мальчиков ходили в спортивный лагерь?'\n",
        "sentence = '   1.4. Во время летних соревнований по плаванию ребята посетили бассейн. Длина плавательной дорожки в бассейне 25 м. После того как первый участник соревнований проплыл часть дорожки, ему осталось проплыть 10 м. Сколько метров уже проплыл участник соревнований?'\n",
        "#sentence = 'Используя данные круговой диаграммы, реши задачу. На диаграмме представлены данные о турпоходе группы. Сколько километров прошла группа в четвёртый день? Введи в поле ответа число без единиц измерения. При необходимости ввести десятичную дробь, разделяй её целую и дробную части запятой, без пробелов.'\n",
        "#sentence = 'Во дворе кот Геннадий охотился на напыщенного толстого голубя. (1) крадущегося кота была равна 1 м/с. Но голубь оказался не так прост и скоро, через (2), заметил приближающегося Геннадия. Птица взлетела на крышу гаража высотой в (3), чтобы отвязаться от кота. Но тот быстро преодолел (4) до этого строения в 5 метров, прыгнул наверх и почти ухватил пернатого за хвост! Голубь, конечно, не стерпел такой наглости и взлетел со скоростью (5). Обиженный Геннадий посмотрел некоторое (6) на улетающую птицу и спрыгнул вниз. Какова длина траектории (в метрах) кота Геннадия за описанную утреннюю прогулку?'\n",
        "#sentence = 'У Миши было 3 три мячика. Два из них он отдал Даше. Сколько мячиков осталось у Миши?'\n",
        "category_from = 2\n",
        "category_to = 0"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0YHuUz1zm4F",
        "outputId": "fd57d8fd-fcd4-4048-fe22-58a8ceb74c36"
      },
      "source": [
        "words_in_sentence = word_tokenize(sentence)\n",
        "for ind, word in enumerate(words_in_sentence):\n",
        "  word = word.lower()\n",
        "  if not word in navec:\n",
        "    continue\n",
        "  pred = word_model.predict(np.array([navec[word]]))[0]\n",
        "  ind_max = np.argmax(pred)\n",
        "  if pred[ind_max].item() > 0.9 and ind_max == category_from:\n",
        "    print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "круговой\n",
            "поле\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "-IqPJ3AoQu2j",
        "outputId": "7ac3fcb9-e755-469d-963c-794527b1f02c"
      },
      "source": [
        "print(sentence)\n",
        "sentence_with_masks, masks = MaskCreator().mask(sentence)\n",
        "#print(sentence_with_masks, masks)\n",
        "new_masks = ['' for i in range(len(masks))]\n",
        "words_in_sentence = word_tokenize(sentence_with_masks)\n",
        "get_first_mask = False\n",
        "for word in words_in_sentence:\n",
        "  if word[:4] == 'mask' and masks[int(word[4:])] in nouns_actors[category_from]:\n",
        "    first_mask = navec[masks[int(word[4:])]]\n",
        "    new_word = nouns_actors[category_to][random.randint(0, 19)]\n",
        "    #new_word = masks[int(word[4:])]\n",
        "    new_first_mask = navec[new_word]\n",
        "    #new_first_mask = first_mask\n",
        "    new_masks[int(word[4:])] = new_word\n",
        "    get_first_mask = True\n",
        "for ind, word in enumerate(words_in_sentence):\n",
        "  if word[:4] == 'mask':\n",
        "    if not get_first_mask:\n",
        "      first_mask = navec[masks[int(word[4:])]]\n",
        "      new_word = nouns[category_to][random.randint(0, 19)]\n",
        "      #new_word = masks[int(word[4:])]\n",
        "      new_first_mask = navec[new_word]\n",
        "      #new_first_mask = first_mask\n",
        "      new_masks[int(word[4:])] = new_word\n",
        "      get_first_mask = True\n",
        "    elif new_masks[int(word[4:])] == '':\n",
        "      min_val = 1\n",
        "      min_new_word = ''\n",
        "      ideal_dist = first_mask - navec[masks[int(word[4:])]]\n",
        "      for new_word in nouns[category_to]:\n",
        "        if not new_word in navec or new_word in new_masks:\n",
        "          continue\n",
        "        now_dist = cos_dist.cosine(navec[new_word], new_first_mask - ideal_dist)\n",
        "        if now_dist < min_val:\n",
        "          min_val = now_dist\n",
        "          min_new_word = new_word\n",
        "      new_masks[int(word[4:])] = min_new_word\n",
        "    words_in_sentence[ind] = new_masks[int(word[4:])]\n",
        "\n",
        "sentence = TreebankWordDetokenizer().detokenize(words_in_sentence)\n",
        "sentence = sentence.replace(' .', '.')\n",
        "\n",
        "sentence"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   1.4. Во время летних соревнований по плаванию ребята посетили бассейн. Длина плавательной дорожки в бассейне 25 м. После того как первый участник соревнований проплыл часть дорожки, ему осталось проплыть 10 м. Сколько метров уже проплыл участник соревнований?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. 4. во время летних кашалот по парусник животное посетили аквариум. длина плавательной речка в аквариум 25 м. после того как первый дельфин кашалот проплыл часть речка, ему осталось проплыть 10 м. сколько метр уже проплыл дельфин кашалот?'"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wqMY1Xl2Zoa"
      },
      "source": [
        "words_in_sentence = word_tokenize(sentence)\n",
        "to_word = {}\n",
        "count_masks = 0\n",
        "for ind, word in enumerate(words_in_sentence):\n",
        "  if morph.parse(word)[0].tag.POS == 'INFN' or morph.parse(word)[0].tag.POS == 'VERB':\n",
        "    words_in_sentence[ind] = '[MASK]'\n",
        "    count_masks += 1\n",
        "    continue\n",
        "  '''\n",
        "  normal_word = morph.parse(word)[0].normal_form\n",
        "  case_word = morph.parse(word)[0].tag.case\n",
        "  number_word = morph.parse(word)[0].tag.number\n",
        "  if normal_word in nouns_actors[category_from]:\n",
        "    if to_word.get(normal_word) == None:\n",
        "      to_word[normal_word] = nouns_actors[category_to][random.randint(0, 19)]\n",
        "    # Заменяю либо на существительное из другой категории, либо на маску (тестировал разные варианты)\n",
        "    #words_in_sentence[ind] = morph.parse(to_word[normal_word])[0].inflect({number_word, case_word})[0]\n",
        "    words_in_sentence[ind] = '[MASK]'\n",
        "    count_masks += 1\n",
        "  if normal_word in nouns[category_from]:\n",
        "    if to_word.get(normal_word) == None:\n",
        "      to_word[normal_word] = nouns[category_to][random.randint(0, 19)]\n",
        "    #words_in_sentence[ind] = morph.parse(to_word[normal_word])[0].inflect({number_word, case_word})[0]\n",
        "    words_in_sentence[ind] = '[MASK]'\n",
        "    count_masks += 1'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "sXxBiew8D8S-",
        "outputId": "4094e062-a51c-43a3-cafe-b805420da69a"
      },
      "source": [
        "sentence = TreebankWordDetokenizer().detokenize(words_in_sentence)\n",
        "sentence = sentence.replace(' .', '.')\n",
        "\n",
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. 4. Во время летних соревнование по океан папа [MASK] пруд. длина плавательной полоса в пруд 25 м. После того как первый художник соревнование [MASK] часть полоса, ему [MASK] [MASK] 10 м. Сколько метр уже [MASK] художник соревнование?'"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHw3_YXBQ0-M"
      },
      "source": [
        "tokenized = tokenizer(sentence, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  outputs = model(**tokenized)\n",
        "masked_index = np.where(torch.flatten(tokenized['input_ids']).numpy() == 103)\n",
        "for i in masked_index[0]:\n",
        "  predicted_index = torch.argmax(outputs[0][0][i]).item()\n",
        "  predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "  ind = sentence.split().index('[MASK]')\n",
        "  sentence = sentence.split()\n",
        "  sentence[ind] = predicted_token\n",
        "  sentence = ' '.join(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "TPiOWj3ASCUT",
        "outputId": "eea3352a-5464-4e34-8c0a-33e2fbc724a2"
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. 4. Во время летних соревнование по океан папа по пруд. длина плавательной полоса в пруд 25 м. После того как первый художник соревнование . часть полоса, ему длина ##о 10 м. Сколько метр уже первый художник соревнование?'"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    }
  ]
}