{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPD5xkHjefGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c81717-2d5a-4ab5-8b6c-fbd1c35bea44"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTfGP2b6Adn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff67753b-9bfc-4703-9829-9bb99c7a8b56"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install pymorphy2[fast]\n",
        "!pip install navec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pymorphy2[fast] in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Requirement already satisfied: DAWG>=0.8 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.8.0)\n",
            "Requirement already satisfied: navec in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRYkmf3b98vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf0f600-dc4a-4233-b05e-72e7950d99c8"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import AdamW, BertForMaskedLM\n",
        "import torch\n",
        "import numpy as np\n",
        "import pymorphy2\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from navec import Navec\n",
        "from typing import List\n",
        "import gensim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import KeyedVectors\n",
        "import scipy.spatial.distance as cos_dist\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(44)\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "nltk.download('punkt')\n",
        "navec = Navec.load(save_path + '/navec_hudlit_v1_12B_500K_300d_100q.tar')\n",
        "save_path = '/content/drive/MyDrive'\n",
        "alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjpegAf1_GJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa71911c-2c60-4518-d65a-6b3f6d719a19"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        " \n",
        "if device == torch.device('cpu'):\n",
        "    print('Using cpu')\n",
        "else:\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print('Using {} GPUs'.format(torch.cuda.get_device_name(0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DRPppmjzk7D"
      },
      "source": [
        "categories = (\"животные\", \"музыка\", \"спорт\", \"литература\")\n",
        "categories_eng = (\"animals\", \"music\", \"sport\", \"literature\")\n",
        "\n",
        "with open(save_path + '/units.txt', 'r') as f:\n",
        "  words = f.read()\n",
        "  units = set(words.split('\\n'))\n",
        "with open(save_path + '/keywords_tasks.txt', 'r') as f:\n",
        "  words = f.read()\n",
        "  keywords_tasks = set(words.split('\\n'))\n",
        "\n",
        "all_nouns_actors = set()\n",
        "nouns_actors = []\n",
        "for ind, cat in enumerate(categories_eng):\n",
        "  with open(save_path + f'/keywords/actors/true_keywords_nouns_actors_{cat}.txt', 'r') as f:\n",
        "    words = f.read()\n",
        "  nouns_actors.append([])\n",
        "  for word in words.split('\\n')[:-1]:\n",
        "    if not word in units:\n",
        "      nouns_actors[ind].append(word)\n",
        "  all_nouns_actors |= set(nouns_actors[ind])\n",
        "\n",
        "all_nouns = set()\n",
        "nouns = []\n",
        "for ind, cat in enumerate(categories_eng):\n",
        "  with open(save_path + f'/keywords/nouns/true_keywords_nouns_{cat}.txt', 'r') as f:\n",
        "    words = f.read()\n",
        "  nouns.append([])\n",
        "  for word in words.split('\\n')[:-1]:\n",
        "    if (not word in units) and (not word in keywords_tasks):\n",
        "      nouns[ind].append(word)\n",
        "  all_nouns |= set(nouns[ind])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XtCRcXnMmzI"
      },
      "source": [
        "class MaskCreator():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.tokenizer = nltk.WordPunctTokenizer()\n",
        "    self.bigram_mod = gensim.models.Phrases.load(save_path + '/bigram_model.pkl')\n",
        "\n",
        "  def make_bigrams(self, doc):\n",
        "      return self.bigram_mod[doc]\n",
        "\n",
        "  def mask(self, text, category=4) -> (str, List[str]):\n",
        "    masks_dict = []\n",
        "    tokens = self.tokenizer.tokenize(text.lower())\n",
        "    tokens_normal = [morph.parse(w)[0].normal_form for w in tokens]\n",
        "    tokens_bigrammed = self.make_bigrams(tokens_normal)\n",
        "    \n",
        "    if len(tokens_bigrammed) < len(tokens):\n",
        "      ind_go = 0\n",
        "      for i in range(len(tokens_bigrammed)):\n",
        "        if tokens_normal[ind_go] != tokens_bigrammed[i]:\n",
        "          tokens = tokens[:ind_go] + [tokens_bigrammed[i]] + tokens[ind_go+2:]\n",
        "          ind_go += 2\n",
        "        else:\n",
        "          ind_go += 1\n",
        "\n",
        "    if category == 4:\n",
        "      now_keywords = all_nouns\n",
        "    else:\n",
        "      now_keywords = nouns[category]\n",
        "\n",
        "    prev_words = []\n",
        "    for ind, token in enumerate(tokens):\n",
        "      word = morph.parse(token.lower())[0].normal_form\n",
        "      if word in now_keywords:\n",
        "        if word not in masks_dict:\n",
        "          masks_dict.append(word)\n",
        "        prev_words.append(tokens[ind])\n",
        "        tokens[ind] = 'mask' + str(masks_dict.index(word, 0))\n",
        "    text = nltk.tokenize.treebank.TreebankWordDetokenizer().detokenize(tokens)\n",
        "    return text, masks_dict, prev_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmFrJ4F4XckZ"
      },
      "source": [
        "all_keywords = [[] for i in range(4)]\n",
        "already_in_keywords = set()\n",
        "for word in navec.vocab.word_ids.keys():\n",
        "  word_normal = morph.parse(word)[0].normal_form\n",
        "  if (not word[0] in alphabet) or morph.parse(word)[0].tag.POS != 'NOUN' or word_normal in already_in_keywords:\n",
        "    continue\n",
        "  to_cats = []\n",
        "  for cat in categories:\n",
        "    to_cats.append(cos_dist.cosine(navec[word], navec[cat]))\n",
        "  ind_min = np.argmin(to_cats)\n",
        "  all_keywords[ind_min].append(word_normal)\n",
        "  already_in_keywords.add(word_normal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MV5oL4CQnNK"
      },
      "source": [
        "#sentence = '4.30. У слона пульс 20 ударов в минуту, а у паука на 40 ударов в минуту больше. Какой пульс у паука?'\n",
        "#sentence = '   1.1. Летом в спортивный лагерь ходили 50 детей, из них 9 девочек. Сколько мальчиков ходили в спортивный лагерь?'\n",
        "#sentence = '   1.4. Во время летних соревнований по плаванию ребята посетили бассейн. Длина плавательной дорожки в бассейне 25 м. После того как первый участник соревнований проплыл часть дорожки, ему осталось проплыть 10 м. Сколько метров уже проплыл участник соревнований?'\n",
        "#sentence = 'Используя данные круговой диаграммы, реши задачу. На диаграмме представлены данные о турпоходе группы. Сколько километров прошла группа в четвёртый день? Введи в поле ответа число без единиц измерения. При необходимости ввести десятичную дробь, разделяй её целую и дробную части запятой, без пробелов.'\n",
        "sentence = 'Во дворе кот Геннадий охотился на напыщенного толстого голубя. (1) крадущегося кота была равна 1 м/с. Но голубь оказался не так прост и скоро, через (2), заметил приближающегося Геннадия. Птица взлетела на крышу гаража высотой в (3), чтобы отвязаться от кота. Но тот быстро преодолел (4) до этого строения в 5 метров, прыгнул наверх и почти ухватил пернатого за хвост! Голубь, конечно, не стерпел такой наглости и взлетел со скоростью (5). Обиженный Геннадий посмотрел некоторое (6) на улетающую птицу и спрыгнул вниз. Какова длина траектории (в метрах) кота Геннадия за описанную утреннюю прогулку?'\n",
        "#sentence = 'У Миши было 3 три мячика. Два из них он отдал Даше. Сколько мячиков осталось у Миши?'\n",
        "category_from = 0\n",
        "category_to = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IqPJ3AoQu2j",
        "outputId": "80330816-d946-4ece-f8ba-3f636931240a"
      },
      "source": [
        "print(sentence)\n",
        "sentence_with_masks, masks, prev_words = MaskCreator().mask(sentence, category_from)\n",
        "print(sentence_with_masks, masks)\n",
        "new_masks = ['' for i in range(len(masks))]\n",
        "words_in_sentence = word_tokenize(sentence_with_masks)\n",
        "get_first_mask = False\n",
        "now_ind_to_prev_ind = {}\n",
        "count_masks = 0\n",
        "for ind, word in enumerate(words_in_sentence):\n",
        "  if word[:4] == 'mask':\n",
        "    now_ind_to_prev_ind[ind] = count_masks\n",
        "    count_masks += 1\n",
        "for word in words_in_sentence:\n",
        "  if word[:4] == 'mask' and masks[int(word[4:])] in nouns_actors[category_from]:\n",
        "    first_mask = navec[masks[int(word[4:])]]\n",
        "    new_word = nouns_actors[category_to][random.randint(0, 3)]\n",
        "    new_first_mask = navec[new_word]\n",
        "    new_masks[int(word[4:])] = new_word\n",
        "    get_first_mask = True\n",
        "for ind, word in enumerate(words_in_sentence):\n",
        "  if word[:4] == 'mask':\n",
        "    if not get_first_mask:\n",
        "      first_mask = navec[masks[int(word[4:])]]\n",
        "      new_word = nouns[category_to][random.randint(0, 19)]\n",
        "      new_first_mask = navec[new_word]\n",
        "      new_masks[int(word[4:])] = new_word\n",
        "      get_first_mask = True\n",
        "    elif new_masks[int(word[4:])] == '':\n",
        "      min_val = 1\n",
        "      min_new_word = ''\n",
        "      #ideal_dist = first_mask - navec[masks[int(word[4:])]]\n",
        "      ideal_dist = navec[masks[int(word[4:])]] - navec[categories[category_from]] + navec[categories[category_to]]\n",
        "      for new_word in nouns[category_to]:\n",
        "        if not new_word in navec or new_word in new_masks:\n",
        "          continue\n",
        "        now_dist = cos_dist.cosine(navec[new_word], new_first_mask - ideal_dist)\n",
        "        if now_dist < min_val:\n",
        "          min_val = now_dist\n",
        "          min_new_word = new_word\n",
        "      if min_val > 0.5:\n",
        "        min_new_word = masks[int(word[4:])]\n",
        "      new_masks[int(word[4:])] = min_new_word\n",
        "    put_word = new_masks[int(word[4:])]\n",
        "    prev_word = prev_words[now_ind_to_prev_ind[ind]]\n",
        "    case_prev_word = morph.parse(prev_word)[0].tag.case\n",
        "    number_prev_word = morph.parse(prev_word)[0].tag.number\n",
        "    words_in_sentence[ind] = morph.parse(put_word)[0].inflect({number_prev_word, case_prev_word})[0]\n",
        "\n",
        "sentence = TreebankWordDetokenizer().detokenize(words_in_sentence)\n",
        "\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Во дворе кот Геннадий охотился на напыщенного толстого голубя. (1) крадущегося кота была равна 1 м/с. Но голубь оказался не так прост и скоро, через (2), заметил приближающегося Геннадия. Птица взлетела на крышу гаража высотой в (3), чтобы отвязаться от кота. Но тот быстро преодолел (4) до этого строения в 5 метров, прыгнул наверх и почти ухватил пернатого за хвост! Голубь, конечно, не стерпел такой наглости и взлетел со скоростью (5). Обиженный Геннадий посмотрел некоторое (6) на улетающую птицу и спрыгнул вниз. Какова длина траектории (в метрах) кота Геннадия за описанную утреннюю прогулку?\n",
            "во mask0 mask1 геннадий охотился на напыщенного толстого голубя . (1) крадущегося mask1 была равна 1 м / с . но голубь оказался не так прост и скоро, через (2 ), заметил приближающегося геннадия . mask2 взлетела на крышу гаража высотой в (3 ), чтобы отвязаться от mask1 . но тот быстро преодолел (4) до этого строения в 5 метров, прыгнул наверх и почти ухватил пернатого за mask3! голубь, конечно, не стерпел такой наглости и взлетел со скоростью (5 ). обиженный геннадий посмотрел некоторое (6) на улетающую mask2 и спрыгнул вниз . какова длина траектории (в метрах) mask1 геннадия за описанную утреннюю прогулку? ['двор', 'кот', 'птица', 'хвост']\n",
            "во дворе спортсмен геннадий охотился на напыщенного толстого голубя . (1) крадущегося спортсмена была равна 1 м / с . но голубь оказался не так прост и скоро, через (2), заметил приближающегося геннадия . игрок взлетела на крышу гаража высотой в (3), чтобы отвязаться от спортсмена . но тот быстро преодолел (4) до этого строения в 5 метров, прыгнул наверх и почти ухватил пернатого за хвост! голубь, конечно, не стерпел такой наглости и взлетел со скоростью (5). обиженный геннадий посмотрел некоторое (6) на улетающую игрока и спрыгнул вниз . какова длина траектории (в метрах) спортсмена геннадия за описанную утреннюю прогулку?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnr0OT3EAj9g",
        "outputId": "33d71450-143b-4659-a7c1-5254c79590bd"
      },
      "source": [
        "cos_dist.cosine(navec['птица'], navec['игрок'] + navec['животные'] - navec['спорт'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7580268085002899"
            ]
          },
          "metadata": {},
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xe8PbHi6o-N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu75Sq8T6o7w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzKRkWR16o5H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0MQ9QTO6oyZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6C6raB6KhYT"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wqMY1Xl2Zoa"
      },
      "source": [
        "words_in_sentence = word_tokenize(sentence)\n",
        "to_word = {}\n",
        "count_masks = 0\n",
        "for ind, word in enumerate(words_in_sentence):\n",
        "  if morph.parse(word)[0].tag.POS == 'INFN' or morph.parse(word)[0].tag.POS == 'VERB':\n",
        "    words_in_sentence[ind] = '[MASK]'\n",
        "    count_masks += 1\n",
        "    continue\n",
        "  '''\n",
        "  normal_word = morph.parse(word)[0].normal_form\n",
        "  case_word = morph.parse(word)[0].tag.case\n",
        "  number_word = morph.parse(word)[0].tag.number\n",
        "  if normal_word in nouns_actors[category_from]:\n",
        "    if to_word.get(normal_word) == None:\n",
        "      to_word[normal_word] = nouns_actors[category_to][random.randint(0, 19)]\n",
        "    # Заменяю либо на существительное из другой категории, либо на маску (тестировал разные варианты)\n",
        "    #words_in_sentence[ind] = morph.parse(to_word[normal_word])[0].inflect({number_word, case_word})[0]\n",
        "    words_in_sentence[ind] = '[MASK]'\n",
        "    count_masks += 1\n",
        "  if normal_word in nouns[category_from]:\n",
        "    if to_word.get(normal_word) == None:\n",
        "      to_word[normal_word] = nouns[category_to][random.randint(0, 19)]\n",
        "    #words_in_sentence[ind] = morph.parse(to_word[normal_word])[0].inflect({number_word, case_word})[0]\n",
        "    words_in_sentence[ind] = '[MASK]'\n",
        "    count_masks += 1'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "sXxBiew8D8S-",
        "outputId": "4094e062-a51c-43a3-cafe-b805420da69a"
      },
      "source": [
        "sentence = TreebankWordDetokenizer().detokenize(words_in_sentence)\n",
        "sentence = sentence.replace(' .', '.')\n",
        "\n",
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. 4. Во время летних соревнование по океан папа [MASK] пруд. длина плавательной полоса в пруд 25 м. После того как первый художник соревнование [MASK] часть полоса, ему [MASK] [MASK] 10 м. Сколько метр уже [MASK] художник соревнование?'"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHw3_YXBQ0-M"
      },
      "source": [
        "tokenized = tokenizer(sentence, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  outputs = model(**tokenized)\n",
        "masked_index = np.where(torch.flatten(tokenized['input_ids']).numpy() == 103)\n",
        "for i in masked_index[0]:\n",
        "  predicted_index = torch.argmax(outputs[0][0][i]).item()\n",
        "  predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "  ind = sentence.split().index('[MASK]')\n",
        "  sentence = sentence.split()\n",
        "  sentence[ind] = predicted_token\n",
        "  sentence = ' '.join(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "TPiOWj3ASCUT",
        "outputId": "eea3352a-5464-4e34-8c0a-33e2fbc724a2"
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. 4. Во время летних соревнование по океан папа по пруд. длина плавательной полоса в пруд 25 м. После того как первый художник соревнование . часть полоса, ему длина ##о 10 м. Сколько метр уже первый художник соревнование?'"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    }
  ]
}