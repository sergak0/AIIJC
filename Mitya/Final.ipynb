{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPD5xkHjefGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a065cdb6-0ed7-43a1-cacc-edf520cc95f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTfGP2b6Adn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd4ceb8-abd7-46fb-ecb7-d83e3edf4269"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install pymorphy2[fast]\n",
        "!pip install navec"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Collecting pymorphy2[fast]\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 5.6 MB/s \n",
            "\u001b[?25hCollecting DAWG>=0.8\n",
            "  Downloading DAWG-0.8.0.tar.gz (371 kB)\n",
            "\u001b[K     |████████████████████████████████| 371 kB 71.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.8.0-cp37-cp37m-linux_x86_64.whl size=857536 sha256=98d1b8c3068885bebda1eed759a65101eaeb5d553a623ee56abdaa481845258a\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/51/a4/2de41ff197786537075027c27b479a38da92f50abc86634445\n",
            "Successfully built DAWG\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2, DAWG\n",
            "Successfully installed DAWG-0.8.0 dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Collecting navec\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec) (1.19.5)\n",
            "Installing collected packages: navec\n",
            "Successfully installed navec-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRYkmf3b98vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f35629-f5a2-4c5c-9bcd-05dc3b0933ce"
      },
      "source": [
        "import numpy as np\n",
        "import pymorphy2\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from navec import Navec\n",
        "from typing import List\n",
        "import gensim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import KeyedVectors\n",
        "import scipy.spatial.distance as cos_dist\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(44)\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "nltk.download('punkt')\n",
        "save_path = '/content/drive/MyDrive'\n",
        "navec = Navec.load(save_path + '/navec_hudlit_v1_12B_500K_300d_100q.tar')\n",
        "alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя ')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DRPppmjzk7D"
      },
      "source": [
        "categories = (\"животные\", \"музыка\", \"спорт\", \"литература\")\n",
        "categories_eng = (\"animals\", \"music\", \"sport\", \"literature\")\n",
        "\n",
        "# Открываем файлы с ключевыми словами и убираем units и keywords_tasks из них\n",
        "\n",
        "with open(save_path + '/units.txt', 'r') as f:\n",
        "  words = f.read()\n",
        "  units = set(words.split('\\n'))\n",
        "with open(save_path + '/keywords_tasks.txt', 'r') as f:\n",
        "  words = f.read()\n",
        "  keywords_tasks = set(words.split('\\n'))\n",
        "\n",
        "all_nouns_actors = set()\n",
        "nouns_actors = []\n",
        "for ind, cat in enumerate(categories_eng):\n",
        "  with open(save_path + f'/keywords/actors/true_keywords_nouns_actors_{cat}.txt', 'r') as f:\n",
        "    words = f.read()\n",
        "  nouns_actors.append([])\n",
        "  for word in words.split('\\n')[:-1]:\n",
        "    if not word in units:\n",
        "      nouns_actors[ind].append(word)\n",
        "  all_nouns_actors |= set(nouns_actors[ind])\n",
        "\n",
        "all_nouns = set()\n",
        "nouns = []\n",
        "for ind, cat in enumerate(categories_eng):\n",
        "  with open(save_path + f'/wow_keywords/wow_keywords_{cat}.txt', 'r') as f:\n",
        "    words = f.read()\n",
        "  nouns.append([])\n",
        "  for word in words.split('\\n')[:-1]:\n",
        "    if (not word in units) and (not word in keywords_tasks):\n",
        "      nouns[ind].append(word)\n",
        "  all_nouns |= set(nouns[ind])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XtCRcXnMmzI"
      },
      "source": [
        "# Делаем маски\n",
        "class MaskCreator():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.tokenizer = nltk.WordPunctTokenizer()\n",
        "    self.bigram_mod = gensim.models.Phrases.load(save_path + '/bigram_model.pkl')\n",
        "\n",
        "  def make_bigrams(self, doc):\n",
        "      return self.bigram_mod[doc]\n",
        "\n",
        "  def mask(self, text, category=4, make_bigrams=False):\n",
        "    masks_dict = []\n",
        "    tokens = self.tokenizer.tokenize(text.lower())\n",
        "    \n",
        "    if make_bigrams:\n",
        "      tokens_normal = [morph.parse(w)[0].normal_form for w in tokens]\n",
        "      tokens_bigrammed = self.make_bigrams(tokens_normal)\n",
        "      \n",
        "      if len(tokens_bigrammed) < len(tokens):\n",
        "        ind_go = 0\n",
        "        for i in range(len(tokens_bigrammed)):\n",
        "          if tokens_normal[ind_go] != tokens_bigrammed[i]:\n",
        "            tokens = tokens[:ind_go] + [tokens_bigrammed[i]] + tokens[ind_go+2:]\n",
        "            ind_go += 2\n",
        "          else:\n",
        "            ind_go += 1\n",
        "\n",
        "    if category == 4:\n",
        "      now_keywords = all_nouns\n",
        "    else:\n",
        "      now_keywords = nouns[category]\n",
        "\n",
        "    prev_words = []\n",
        "    for ind, token in enumerate(tokens):\n",
        "      word = morph.parse(token.lower())[0].normal_form\n",
        "      if word in now_keywords:\n",
        "        if word not in masks_dict:\n",
        "          masks_dict.append(word)\n",
        "        prev_words.append(tokens[ind])\n",
        "        tokens[ind] = 'mask' + str(masks_dict.index(word, 0))\n",
        "    text = nltk.tokenize.treebank.TreebankWordDetokenizer().detokenize(tokens)\n",
        "    return text, masks_dict, prev_words"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgYbZkz0Mm5_"
      },
      "source": [
        "'''check_words = [['животные', 'животное', 'собака', 'кошка', 'млекопитающее', 'птица', 'зверь', 'хищник', 'паразит', 'бактерия'],\n",
        "               ['музыка', 'нота', 'ноты', 'песня', 'октава', 'музыкант', 'звучать', 'петь', 'пианино', 'гитара'],\n",
        "               ['спорт', 'соревнование', 'побеждать', 'победа', 'матч', 'стадион', 'спортсмен', 'болельщик', 'сборная', 'выиграть'],\n",
        "               ['литература', 'книга', 'страница', 'писатель', 'чтение', 'читать', 'читатель', 'газета', 'библиотека', 'книжка']]'''\n",
        "check_words = [['животные', 'животное', 'зверь'],\n",
        "               ['музыка', 'нота', 'звучать'],\n",
        "               ['спорт', 'побеждать', 'спортсмен'],\n",
        "               ['литература', 'книга', 'читать']]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmFrJ4F4XckZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbbd03e-da5f-4180-9fb3-58eea6ce2c3e"
      },
      "source": [
        "# Создаём качественные ключевые слова\n",
        "\n",
        "all_keywords = [[] for i in range(4)]\n",
        "already_in_keywords = set()\n",
        "for word in tqdm(navec.vocab.word_ids.keys()):\n",
        "  word_normal = morph.parse(word)[0].normal_form\n",
        "  if (not word[0] in alphabet) or word_normal in already_in_keywords:\n",
        "    continue\n",
        "  min_check_words = [1, 1, 1, 1]\n",
        "  max_check_words = [0, 0, 0, 0]\n",
        "  for ind_cat in range(4):\n",
        "    for check_word in check_words[ind_cat]:\n",
        "      now_cos_dist = cos_dist.cosine(navec[word], navec[check_word])\n",
        "      min_check_words[ind_cat] = min(min_check_words[ind_cat], now_cos_dist)\n",
        "      max_check_words[ind_cat] = max(max_check_words[ind_cat], now_cos_dist)\n",
        "  ind_min = -1\n",
        "  for ind_cat in range(4):\n",
        "    max_lower_mins = True\n",
        "    for last_ind_cat in range(4):\n",
        "      if ind_cat == last_ind_cat:\n",
        "        continue\n",
        "      if max_check_words[ind_cat] > min_check_words[last_ind_cat]: # Если маскимальное расстояние от слова до всех векторов из одной категории меньше, чем минимальное расстояние до всех остальных, то считаем его ключевым\n",
        "        max_lower_mins = False\n",
        "        break\n",
        "    if max_lower_mins:\n",
        "      ind_min = ind_cat\n",
        "      break\n",
        "  if ind_min != -1:\n",
        "    all_keywords[ind_min].append(word_normal)\n",
        "    already_in_keywords.add(word_normal)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500002/500002 [11:32<00:00, 721.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj4v5jZ2d8dK"
      },
      "source": [
        "for ind, cat in enumerate(categories_eng):\n",
        "  with open(save_path + f'/wow_keywords/wow_keywords_{cat}.txt', 'w') as f:\n",
        "    f.write('\\n'.join(list(set(all_keywords[ind])|set(nouns[ind]))))\n",
        "    #f.write('\\n'.join(all_keywords[ind]))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvdZY2fILm_7"
      },
      "source": [
        "def nearest_word(ideal_dist, list_with_words, ban_words=[]): # Ближайшее слово из (list_with_words без ban_words) к вектору ideal_dist\n",
        "  min_val = 1\n",
        "  min_new_word = ''\n",
        "  for new_word in list_with_words:\n",
        "    if (not new_word in navec) or (new_word in ban_words):\n",
        "      continue\n",
        "    now_dist = cos_dist.cosine(navec[new_word], ideal_dist)\n",
        "    if now_dist < min_val:\n",
        "      min_val = now_dist\n",
        "      min_new_word = new_word\n",
        "  return min_val, min_new_word"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47klXRAurw8f"
      },
      "source": [
        "def translate(sentence, category_from, category_to, all_print=False):\n",
        "  if all_print:\n",
        "    print(sentence)\n",
        "  sentence_with_masks, masks, prev_words = MaskCreator().mask(sentence, category_from)\n",
        "  if all_print:\n",
        "    print(sentence_with_masks, masks)\n",
        "  \n",
        "  new_masks = ['' for i in range(len(masks))]\n",
        "  words_in_sentence = word_tokenize(sentence_with_masks)\n",
        "  get_first_mask = False\n",
        "  now_ind_to_prev_ind = {}\n",
        "  count_masks = 0\n",
        "  for ind, word in enumerate(words_in_sentence):\n",
        "    if word[:4] == 'mask':\n",
        "      now_ind_to_prev_ind[ind] = count_masks\n",
        "      count_masks += 1\n",
        "\n",
        "  # Подбираем первое слово (подлежащее)\n",
        "  for word in words_in_sentence:\n",
        "    if word[:4] == 'mask' and (masks[int(word[4:])] in nouns_actors[category_from]) and (masks[int(word[4:])] in navec):\n",
        "      first_mask = navec[masks[int(word[4:])]]\n",
        "      min_val, min_new_word = nearest_word(first_mask - navec[categories[category_from]] + navec[categories[category_to]], nouns_actors[category_to])\n",
        "      new_word = min_new_word\n",
        "      new_first_mask = navec[new_word]\n",
        "      new_masks[int(word[4:])] = new_word\n",
        "      get_first_mask = True\n",
        "      break\n",
        "\n",
        "  # Подбираем все остальные слова\n",
        "  for ind, word in enumerate(words_in_sentence):\n",
        "    if word[:4] == 'mask':\n",
        "      if not masks[int(word[4:])] in navec:\n",
        "        new_masks[int(word[4:])] = masks[int(word[4:])]\n",
        "      # Если первое слово ещё не выбрано\n",
        "      if (not get_first_mask) and (masks[int(word[4:])] in navec):\n",
        "        first_mask = navec[masks[int(word[4:])]]\n",
        "        min_val, min_new_word = nearest_word(first_mask - navec[categories[category_from]] + navec[categories[category_to]], nouns[category_to])\n",
        "        new_word = min_new_word\n",
        "        new_first_mask = navec[new_word]\n",
        "        new_masks[int(word[4:])] = new_word\n",
        "        get_first_mask = True\n",
        "      elif new_masks[int(word[4:])] == '' and (masks[int(word[4:])] in navec):\n",
        "        min_val, min_new_word = nearest_word(new_first_mask - (first_mask - navec[masks[int(word[4:])]]), nouns[category_to], new_masks)\n",
        "        if min_val > 0.64: # Если полученное слово не очень похоже на слово, которе должно там стоять, не меняем это слово\n",
        "          min_new_word = masks[int(word[4:])]\n",
        "        new_masks[int(word[4:])] = min_new_word\n",
        "      put_word = new_masks[int(word[4:])]\n",
        "      prev_word = prev_words[now_ind_to_prev_ind[ind]]\n",
        "      case_prev_word = morph.parse(prev_word)[0].tag.case\n",
        "      number_prev_word = morph.parse(prev_word)[0].tag.number\n",
        "      # Ставим в исходную форму, если можем (в форму в первом предложении)\n",
        "      try:\n",
        "        words_in_sentence[ind] = morph.parse(put_word)[0].inflect({number_prev_word, case_prev_word})[0]\n",
        "      except:\n",
        "        words_in_sentence[ind] = put_word\n",
        "\n",
        "  sentence = TreebankWordDetokenizer().detokenize(words_in_sentence)\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IqPJ3AoQu2j",
        "outputId": "70d75b09-f0d3-4b3b-fc44-00c2ed21d2c5"
      },
      "source": [
        "#sentence = '4.30. У слона пульс 20 ударов в минуту, а у паука на 40 ударов в минуту больше. Какой пульс у паука?'\n",
        "#sentence = '   1.1. Летом в спортивный лагерь ходили 50 детей, из них 9 девочек. Сколько мальчиков ходили в спортивный лагерь?'\n",
        "#sentence = '   1.4. Во время летних соревнований по плаванию ребята посетили бассейн. Длина плавательной дорожки в бассейне 25 м. После того как первый участник соревнований проплыл часть дорожки, ему осталось проплыть 10 м. Сколько метров уже проплыл участник соревнований?'\n",
        "#sentence = 'Используя данные круговой диаграммы, реши задачу. На диаграмме представлены данные о турпоходе группы. Сколько километров прошла группа в четвёртый день? Введи в поле ответа число без единиц измерения. При необходимости ввести десятичную дробь, разделяй её целую и дробную части запятой, без пробелов.'\n",
        "#sentence = 'Во дворе кот Геннадий охотился на напыщенного толстого голубя. (1) крадущегося кота была равна 1 м/с. Но голубь оказался не так прост и скоро, через (2), заметил приближающегося Геннадия. Птица взлетела на крышу гаража высотой в (3), чтобы отвязаться от кота. Но тот быстро преодолел (4) до этого строения в 5 метров, прыгнул наверх и почти ухватил пернатого за хвост! Голубь, конечно, не стерпел такой наглости и взлетел со скоростью (5). Обиженный Геннадий посмотрел некоторое (6) на улетающую птицу и спрыгнул вниз. Какова длина траектории (в метрах) кота Геннадия за описанную утреннюю прогулку?'\n",
        "#sentence = 'У Миши было 3 три мячика. Два из них он отдал Даше. Сколько мячиков осталось у Миши?'\n",
        "sentence = 'Как ты уже знаешь, для того чтобы транспортное средство перемещалось с большой скоростью, ему необходимо ускоряться. Будь то самолет, поезд или автомобиль, люди часто готовы рисковать своей жизнью ради победы в гонке или мирового рекорда. Подготовь выступление о 10 самых интересных быстрых транспортных средствах, когда-либо созданных человеком. Оцени ускорение, с которым они разгонялись до максимальной скорости. Результат представь в виде презентации.'\n",
        "category_from = 2\n",
        "category_to = 1\n",
        "\n",
        "print(translate(sentence, category_from, category_to, True))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Как ты уже знаешь, для того чтобы транспортное средство перемещалось с большой скоростью, ему необходимо ускоряться. Будь то самолет, поезд или автомобиль, люди часто готовы рисковать своей жизнью ради победы в гонке или мирового рекорда. Подготовь выступление о 10 самых интересных быстрых транспортных средствах, когда-либо созданных человеком. Оцени ускорение, с которым они разгонялись до максимальной скорости. Результат представь в виде презентации.\n",
            "как ты уже знаешь, для того чтобы транспортное средство перемещалось с большой скоростью, ему необходимо ускоряться . будь то самолет, поезд или mask0, люди часто готовы рисковать своей жизнью ради mask1 в mask2 или мирового mask3 . подготовь mask4 о 10 самых интересных mask5 транспортных средствах, когда - либо созданных человеком . оцени ускорение, с которым они разгонялись до mask6 скорости . результат представь в виде презентации. ['автомобиль', 'победа', 'гонка', 'рекорд', 'выступление', 'быстрый', 'максимальный']\n",
            "как ты уже знаешь, для того чтобы транспортное средство перемещалось с большой скоростью, ему необходимо ускоряться . будь то самолет, поезд или музыка, люди часто готовы рисковать своей жизнью ради песни в мелодии или мирового концерта . подготовь выступление о 10 самых интересных ритмов транспортных средствах, когда - либо созданных человеком . оцени ускорение, с которым они разгонялись до максимального скорости . результат представь в виде презентации.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuPMGPk3sw0_"
      },
      "source": [
        "df_tasks = pd.read_csv(save_path + '/dataset_disclosed.csv', sep=';')\n",
        "df_ans = pd.read_csv(save_path + '/test_finals.csv', sep=';')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emDRVn_ytb4f"
      },
      "source": [
        "df_tasks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtpH---2tckP"
      },
      "source": [
        "df_ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-B22ykktkIn",
        "outputId": "10e9f634-2a7f-48b8-e7f0-3a55c7cb891a"
      },
      "source": [
        "for ind in tqdm(range(len(df_tasks['task']))):\n",
        "  category_from = categories.index(df_tasks['category'][ind])\n",
        "  for category_to in range(4):\n",
        "    if category_to == category_from:\n",
        "      df_ans[categories_eng[category_to]][ind] = df_tasks['task'][ind]\n",
        "    else:\n",
        "      df_ans[categories_eng[category_to]][ind] = translate(df_tasks['task'][ind], category_from, category_to)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/514 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            " 74%|███████▍  | 380/514 [42:47<19:37,  8.78s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaiKbwe1upU_"
      },
      "source": [
        "df_ans.to_csv(save_path + '/ans.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}